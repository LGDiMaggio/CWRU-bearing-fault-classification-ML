{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Module Path\n",
    "\n",
    "Before we begin with the data processing and analysis, it is crucial to ensure that our notebook has access to custom modules stored in different directories. The following code snippet adjusts the Python path to include the 'src' directory where our custom modules are located. This step is necessary to enable the import and use of modules developed specifically for this project without encountering import errors.\n",
    "\n",
    "By inserting the path to the 'src' directory into `sys.path`, we seamlessly import any module from the 'src' directory as if they were part of the standard library or local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the module path is included in the PYTHONPATH\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to the 'src' directory relative to the notebook\n",
    "src_path = os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "\n",
    "# Check if the path is already in sys.path, if not, add it\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the CWRU Bearing Dataset\n",
    "\n",
    "In this section, we retrieve the CWRU (Case Western Reserve University) Bearing Dataset from Kaggle. This dataset is commonly used for predictive maintenance and fault diagnosis in bearings, which is crucial for industrial applications. The function `download_dataset` is imported from the `cwru_data_loading` module, which we previously included in our path.\n",
    "\n",
    "**Steps to Set Up Kaggle API for Download:**\n",
    "\n",
    "1. **Obtain API Credentials from Kaggle:**\n",
    "   - Go to your Kaggle account, navigate to the 'Account' tab from your profile.\n",
    "   - Scroll to the 'API' section and click on 'Create New API Token'.\n",
    "   - This will download a `kaggle.json` file containing your API credentials.\n",
    "\n",
    "2. **Configure the Environment Variable:**\n",
    "   - Place the `kaggle.json` file in a location on your machine.\n",
    "   - Set the `KAGGLE_CONFIG_DIR` environment variable to point to the folder containing `kaggle.json`.\n",
    "   - For example, if your `kaggle.json` is in `C:\\Users\\Username\\.kaggle\\`, you would set the environment variable like this on Windows:\n",
    "     ```bash\n",
    "     setx KAGGLE_CONFIG_DIR \"C:\\Users\\Username\\.kaggle\\\"\n",
    "     ```\n",
    "   - On Unix systems, add the export to your shell configuration file, like `.bashrc` or `.zshrc`:\n",
    "     ```bash\n",
    "     export KAGGLE_CONFIG_DIR=\"/home/username/.kaggle/\"\n",
    "     ```\n",
    "\n",
    "By setting up the Kaggle API and using the provided function, the dataset is automatically downloaded and unzipped to the specified local directory `../data`, ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from the module\n",
    "from cwru_data_loading import download_dataset\n",
    "\n",
    "# Download the dataset to the specified folder\n",
    "download_dataset('brjapon/cwru-bearing-datasets', '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset from Raw Files\n",
    "\n",
    "After downloading the CWRU dataset, we proceed to construct a structured dataset ready for analysis. The `build_dataset` function is used to parse the raw `.mat` files and extract significant features from the bearing vibration signals, making it ready for machine learning applications.\n",
    "\n",
    "**Steps Involved in Dataset Construction:**\n",
    "\n",
    "1. **Load Signal Data:**\n",
    "   - The function reads through `.mat` files, extracting Drive End (DE) and Fan End (FE) vibration signal data.\n",
    "   \n",
    "2. **Segmentation:**\n",
    "   - Each signal is segmented into frames of `2048` points with `50%` overlap between consecutive frames. This segmentation helps in analyzing the signal in manageable parts, capturing transient characteristics.\n",
    "\n",
    "3. **Feature Extraction:**\n",
    "   - For each segment, time-domain and frequency-domain features are extracted:\n",
    "     - Time-domain features might include measures like mean, standard deviation, skewness, etc.\n",
    "     - Frequency-domain features may cover aspects like mean frequency, spectral kurtosis, etc.\n",
    "   - The type of features extracted (`time`, `frequency`, or `both`) depends on the `feature_type` parameter.\n",
    "\n",
    "4. **Data Structuring:**\n",
    "   - Each data point is labeled with its corresponding `specific_label`, `health_state`, `damage_size`, and `signal_type` derived from the file's name.\n",
    "   - A comprehensive DataFrame is constructed, which includes labels and extracted features for each segment.\n",
    "\n",
    "5. **Saving the Data:**\n",
    "   - The constructed DataFrame is saved as a CSV file for easy access and manipulation in future analyses.\n",
    "   - This allows for a standardized dataset that can be used across different models and studies.\n",
    "\n",
    "**Outputs Generated:**\n",
    "- The first few records of the dataset are displayed to verify its structure.\n",
    "- A statistical description of the dataset is printed to understand the distribution of features.\n",
    "- The total number of rows and the occurrences of each class within key columns (`Health State` and `Specific Label`) are shown to give insights into the dataset's composition.\n",
    "\n",
    "By completing these steps, we ensure that the data is not only accessible but also arranged in a way that is conducive to machine learning workflows, paving the way for in-depth analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset using the downloaded data\n",
    "from cwru_data_loading import build_dataset\n",
    "\n",
    "dataset, features_list = build_dataset('../data/raw/', frame_size=2048, overlap=0.5, feature_type='both')\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(dataset.head())\n",
    "\n",
    "# Statistical description of the numerical columns\n",
    "print(\"\\nStatistical Description:\")\n",
    "print(dataset.describe())\n",
    "\n",
    "# Total number of rows in the dataset\n",
    "print(\"\\nTotal number of rows:\", len(dataset))\n",
    "\n",
    "# Count the occurrences for each class in the 'Health State' column\n",
    "print(\"\\nCount of occurrences for each class:\")\n",
    "print(dataset['Health State'].value_counts())\n",
    "\n",
    "# Count the occurrences for each class in the 'Specific Label' column\n",
    "print(\"\\nCount of occurrences for each class:\")\n",
    "print(dataset['Specific Label'].value_counts())\n",
    "\n",
    "# Path to save the CSV file\n",
    "csv_path = '../data/CWRUdataset.csv'\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nThe dataset has been saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Processed Dataset\n",
    "\n",
    "In scenarios where the dataset has already been processed and saved, we can directly load the data from a CSV file. This approach is particularly useful when the data preparation step is time-consuming and we want to avoid repeating the process unnecessarily.\n",
    "\n",
    "**Procedure:**\n",
    "- The dataset is loaded into a DataFrame from the CSV file located at `..\\data\\CWRUdataset.csv`. This file contains the structured data with features and labels prepared from the raw `.mat` files.\n",
    "- Using `pandas.read_csv`, we ensure that the data is quickly and efficiently loaded into the memory, ready for analysis and model training.\n",
    "\n",
    "This method is efficient for repeated analyses, allowing for immediate access to the data without the need for re-processing the raw files.\n",
    "\n",
    "**Note:** This step assumes that the dataset has been previously created and saved correctly in the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('..\\data\\CWRUdataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Data visualization is a crucial step in understanding the underlying patterns and distributions within the dataset. By visually representing the data, we can gain insights into the characteristics of different features and how they relate to different health states and signal types of bearings.\n",
    "\n",
    "**Visualizations Included:**\n",
    "1. **Histograms:** For a selected subset of features, histograms are generated to observe the frequency distribution. This helps in understanding the spread and central tendency of each feature across different specific labels.\n",
    "   - Features visualized include Mean, Standard Deviation, Mean Absolute Deviation, Root Mean Square, Maximum Absolute Value, Skewness, Kurtosis, Crest Factor, Form Factor, Shape Factor, and Impulse Factor.\n",
    "\n",
    "2. **Violin Plots:** For a comprehensive view, violin plots for all the features are created to analyze the distribution of values segmented by health state and further split by signal type. This allows us to see the distribution shapes and the prevalence of values, providing insights into the variability and typical values across different conditions.\n",
    "\n",
    "**Technical Setup:**\n",
    "- The visualizations leverage the `seaborn` library for plotting, known for its aesthetic presentation and advanced plotting options.\n",
    "- Each plot is adjusted for better visibility with a set theme and font sizes ensuring that each visualization is clear and interpretable.\n",
    "- Histograms and violin plots are created using specific functions defined in the `data_visualization` module, which abstracts the complexity of `seaborn` and `matplotlib` setup, making the visualization process straightforward and repeatable.\n",
    "\n",
    "These visualizations are crucial for the preliminary analysis and ensure that the features selected for model training are understood in terms of their distribution and impact on the models' ability to differentiate between different states of health in bearings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['Mean', 'Standard deviation', 'Mean absolute deviation', 'Root mean square',\n",
    "                    'Maximum absolute value', 'Skewness', 'Kurtosis', 'Crest factor', 'Form factor',\n",
    "                    'Shape factor', 'Impulse factor', 'Mean of power spectrum', 'Standard deviation of power spectrum',\n",
    "                    'Skewness of power spectrum', 'Kurtosis of power spectrum', 'Mean frequency', 'Standard deviation of frequency',\n",
    "                    'Root mean square frequency', 'Root variance frequency', 'Frequency centroid', 'Frequency variation factor',\n",
    "                    'Frequency variance', 'Frequency skewness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from the module\n",
    "from data_visualization import plot_histogram, plot_violin_plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path for the results directory\n",
    "results_dir = '..\\Results'\n",
    "\n",
    "# Create the Results directory if it doesn't exist\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "histograms_dir = os.path.join(results_dir, 'Features_Histograms')   \n",
    "violinplots_dir = os.path.join(results_dir, 'Features_Violins')\n",
    "\n",
    "if not os.path.exists(histograms_dir):\n",
    "    os.makedirs(histograms_dir)\n",
    "\n",
    "if not os.path.exists(violinplots_dir):\n",
    "    os.makedirs(violinplots_dir)\n",
    "\n",
    "features_to_plot_hist = ['Mean', 'Standard deviation', 'Mean absolute deviation', 'Root mean square',\n",
    "                    'Maximum absolute value', 'Skewness', 'Kurtosis', 'Crest factor', 'Form factor',\n",
    "                    'Shape factor', 'Impulse factor']\n",
    "\n",
    "features_to_plot_violin = features_list\n",
    "\n",
    "for feature in features_to_plot_hist:\n",
    "    plot_histogram(dataset, feature)\n",
    "    plt.savefig(os.path.join(histograms_dir, f'histogram_{feature}.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "for feature in features_to_plot_violin:\n",
    "    plot_violin_plot(dataset, feature)\n",
    "    plt.savefig(os.path.join(violinplots_dir, f'violinplot_{feature}.png'))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Dimensionality Reduction\n",
    "\n",
    "To visualize the high-dimensional data in a way that is easier to interpret, we utilize t-Distributed Stochastic Neighbor Embedding (t-SNE), a powerful tool for reducing the dimensionality of data while preserving the relative distances between points. This section details the process of applying t-SNE to the CWRU bearing dataset.\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Feature Selection:** We select all the features extracted and listed during the dataset building process for dimensionality reduction.\n",
    "2. **Standardization:** Given the variance in magnitude across different features, we standardize the features to have a mean of zero and a standard deviation of one. This normalization ensures that all features contribute equally to the analysis and improves the t-SNE outcome.\n",
    "3. **t-SNE Execution:** We apply t-SNE to the standardized features, reducing the dataset to two components. This reduction allows us to plot and visually inspect the dataset in two dimensions, seeking patterns or clusters that relate to different operational conditions and health states of the bearings.\n",
    "4. **Incorporation into Dataset:** The two-dimensional t-SNE results are added back into the dataset as new features (`tsne-2d-one` and `tsne-2d-two`). This enables easy plotting and further analysis in subsequent steps.\n",
    "\n",
    "**Purpose of t-SNE Visualization:**\n",
    "- **Cluster Identification:** t-SNE helps in identifying clusters in the data, which can correspond to different health states or operational conditions in the bearing data. Observing how data points cluster can guide the interpretation of the model outputs and help in diagnosing bearing conditions.\n",
    "- **Outlier Detection:** By visualizing the data in two dimensions, we can easily spot anomalies or outliers which might indicate errors in data collection, unusual bearing conditions, or faults.\n",
    "\n",
    "This visualization not only aids in understanding the data's underlying structure but also enhances our ability to communicate complex relationships and patterns in a simple and effective manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features from the dataset\n",
    "features = dataset[features_list]  # Use feature_names obtained from build_dataset\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state=42)\n",
    "tsne_results = tsne.fit_transform(features_scaled)\n",
    "\n",
    "# Add t-SNE results to the dataset\n",
    "dataset['tsne-2d-one'] = tsne_results[:, 0]\n",
    "dataset['tsne-2d-two'] = tsne_results[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of t-SNE Results\n",
    "\n",
    "In this section, we visualize the results of the t-SNE dimensionality reduction using two scatter plots. The goal is to understand how the different classes are distributed in the reduced two-dimensional space.\n",
    "\n",
    "#### t-SNE with Specific Labels\n",
    "\n",
    "The first plot represents the data points colored according to the 'Specific Label', which combines the health state and the signal type (DE or FE). This visualization helps us identify the clusters formed by different specific labels in the t-SNE space.\n",
    "\n",
    "- **Figure Setup**: We set the figure size to (16, 10) to ensure the plot is clear and readable.\n",
    "- **Color Palette**: The \"hsv\" color palette is used to distinguish between the unique specific labels, ensuring each label has a distinct color.\n",
    "- **Plot Details**: We add titles and labels to the axes, and adjust the font sizes for readability. The legend shows all unique specific labels with corresponding colors.\n",
    "\n",
    "#### t-SNE with Health State and Signal Type\n",
    "\n",
    "The second plot shows the data points colored according to the 'Health State with Signal', which provides a combination of health state and signal type. This helps visualize how the signal type influences the distribution of health states.\n",
    "\n",
    "- **Combined Label**: We create a new column 'Health State with Signal' that combines the 'Health State' and 'Signal Type'.\n",
    "- **Color Palette**: The \"tab20\" color palette is used to distinguish the unique health state and signal type combinations, providing a clear distinction between different categories.\n",
    "- **Plot Details**: Similar to the previous plot, titles, axis labels, and legend details are included with appropriate font sizes.\n",
    "\n",
    "These visualizations provide insights into how the data clusters in the reduced dimensional space, offering a visual understanding of the separability of different classes based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path for the results directory\n",
    "results_dir = '../Results/tSNE'\n",
    "\n",
    "# Create the Results directory if it doesn't exist\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "# Plot t-SNE results with specific labels\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.scatterplot(\n",
    "    x='tsne-2d-one', y='tsne-2d-two',\n",
    "    hue='Specific Label',\n",
    "    palette=sns.color_palette(\"hsv\", len(dataset['Specific Label'].unique())),\n",
    "    data=dataset,\n",
    "    legend='full',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('CWRU 1hp data - t-SNE', fontsize=20)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=18)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(fontsize=14, title_fontsize=16)\n",
    "plt.savefig(os.path.join(results_dir, f'tSNE_FullLabel.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Create a combined column for Health State and Signal Type\n",
    "dataset['Health State with Signal'] = dataset['Health State'] + ' ' + dataset['Signal Type']\n",
    "\n",
    "# Determine the number of unique categories for better palette selection\n",
    "unique_categories = dataset['Health State with Signal'].nunique()\n",
    "palette_choice = sns.color_palette(\"tab20\")\n",
    "\n",
    "# Plot t-SNE results with Health State and Signal Type\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.scatterplot(\n",
    "    x='tsne-2d-one', y='tsne-2d-two',\n",
    "    hue='Health State with Signal',\n",
    "    palette=palette_choice,\n",
    "    data=dataset,\n",
    "    legend='full',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('CWRU 1hp data health states - t-SNE', fontsize=20)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=18)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(fontsize=14, title_fontsize=16)\n",
    "plt.savefig(os.path.join(results_dir, f'tSNE_HealthState.png'))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Heatmap\n",
    "\n",
    "In this section, we visualize the correlation between different features in the dataset using a heatmap. Correlation helps us understand the linear relationships between pairs of features, which can be important for feature selection and dimensionality reduction.\n",
    "\n",
    "#### Heatmap Details\n",
    "\n",
    "- **Correlation Matrix**: We calculate the correlation matrix using the features in `features_list`. This matrix quantifies the linear relationship between each pair of features, with values ranging from -1 to 1. A value of 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
    "\n",
    "- **Plot Setup**: \n",
    "  - The heatmap is plotted using the `seaborn` library, which provides an intuitive and visually appealing way to represent the correlation matrix.\n",
    "  - The color map `coolwarm` is used to represent the range of correlations, where warmer colors indicate positive correlations and cooler colors indicate negative correlations.\n",
    "\n",
    "- **Annotations**: Numerical annotations are omitted for clarity. Instead, the focus is on the color gradient, which provides a quick visual assessment of the relationships.\n",
    "\n",
    "- **Figure Size**: The figure size is set to (12, 10) to ensure the heatmap is displayed clearly and legibly.\n",
    "\n",
    "This heatmap is a crucial step in exploratory data analysis, as it highlights potential multicollinearity issues and guides the selection of features for subsequent modeling efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path for the results directory\n",
    "results_dir = '../Results/Correlations'\n",
    "\n",
    "# Create the Results directory if it doesn't exist\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = dataset[features_list].corr()\n",
    "\n",
    "# Create the heatmap without numerical annotations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=20)\n",
    "plt.savefig(os.path.join(results_dir, 'FeatureCorrelations.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "\n",
    "In this section, we employ Principal Component Analysis (PCA) to reduce the dimensionality of our feature space while retaining a significant portion of the dataset's variance. This step is crucial for simplifying the model without losing critical information.\n",
    "\n",
    "#### Process Overview\n",
    "\n",
    "1. **Standardization**:\n",
    "   - We first standardize the features using `StandardScaler`. Standardization ensures that each feature contributes equally to the PCA by centering the data around a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Label Encoding**:\n",
    "   - The `Health State with Signal` labels are encoded using `LabelEncoder`. This step converts categorical labels into numerical format, which is necessary for machine learning algorithms that require numerical input.\n",
    "\n",
    "3. **PCA Application**:\n",
    "   - PCA is applied to the standardized features with the goal of retaining 90% of the total variance. The parameter `n_components=0.9` automatically selects the number of principal components needed to achieve this variance retention.\n",
    "   - The resulting principal components provide a compressed representation of the data, reducing complexity and potential overfitting in subsequent models.\n",
    "\n",
    "4. **Output**:\n",
    "   - The number of principal components used is printed to give insight into the dimensionality reduction achieved. This helps us understand how many features are sufficient to explain most of the variance in the dataset.\n",
    "\n",
    "PCA is a powerful tool for dimensionality reduction, especially when dealing with datasets with a large number of correlated features. It helps improve model performance and computational efficiency while minimizing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(dataset[features_list])\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(dataset['Health State with Signal'])\n",
    "\n",
    "# Apply PCA to retain 90% of the variance\n",
    "explain_variance = 0.9\n",
    "pca = PCA(n_components=explain_variance)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "print(f\"PCA is using {pca.n_components_} components to cover {explain_variance*100}% of dataset variance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "In this section, we focus on evaluating the performance of different machine learning models for classifying the bearing health states. We employ a Bayesian hyperparameter optimization strategy to fine-tune each model, ensuring that we achieve optimal performance.\n",
    "\n",
    "#### Models and Hyperparameter Search\n",
    "\n",
    "1. **Models Considered**:\n",
    "   - **Support Vector Machine (SVM)**: A powerful classifier capable of handling high-dimensional spaces.\n",
    "   - **k-Nearest Neighbors (kNN)**: A simple yet effective algorithm based on proximity to training examples.\n",
    "   - **XGBoost**: An efficient and scalable implementation of gradient boosting that is well-suited for structured data.\n",
    "\n",
    "2. **Bayesian Hyperparameter Optimization**:\n",
    "   - We use `BayesSearchCV` from the `skopt` library to perform hyperparameter optimization.\n",
    "   - For each model, we define a search space of hyperparameters and execute 50 iterations of the search process.\n",
    "   - The scoring metric used for optimization is accuracy.\n",
    "\n",
    "3. **Scoring Metrics**:\n",
    "   - We evaluate models using several metrics calculated through cross-validation:\n",
    "     - **Accuracy**: The fraction of correctly classified instances.\n",
    "     - **Precision (Micro-averaged)**: The fraction of relevant instances among the retrieved instances.\n",
    "     - **Recall (Micro-averaged)**: The fraction of relevant instances that were retrieved.\n",
    "     - **F1 Score (Micro-averaged)**: The harmonic mean of precision and recall, providing a balance between them.\n",
    "\n",
    "4. **Evaluation Process**:\n",
    "   - We assess each model's performance both with and without Principal Component Analysis (PCA).\n",
    "   - For each configuration, we calculate the mean and standard deviation for each scoring metric using cross-validation.\n",
    "   - Results are compiled into a comprehensive table for comparison.\n",
    "\n",
    "5. **Results Storage**:\n",
    "   - The final results table is saved as a CSV file for further analysis and reporting. The results include the average scores and standard deviations for accuracy, precision, recall, and F1 score, categorized by model and feature set (with or without PCA).\n",
    "\n",
    "This analysis provides a thorough comparison of different machine learning models and feature sets, highlighting the impact of dimensionality reduction through PCA on classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define models and parameters for Bayesian search\n",
    "models = {\n",
    "    'SVM': SVC(),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='mlogloss'),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'SVM': {'C': (0.1, 100, 'log-uniform'), 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
    "    'kNN': {'n_neighbors': (1, 20), 'metric': ['euclidean', 'manhattan']},\n",
    "    'XGBoost': {'max_depth': (3, 10), 'learning_rate': (0.01, 0.2, 'log-uniform'), 'n_estimators': (50, 300)},\n",
    "}\n",
    "\n",
    "# Scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score), \n",
    "    'precision_micro': make_scorer(precision_score, average='micro'),\n",
    "    'recall_micro': make_scorer(recall_score, average='micro'),\n",
    "    'f1_micro': make_scorer(f1_score, average='micro')\n",
    "}\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(name, model, features, labels):\n",
    "    optimizer = BayesSearchCV(model, params[name], n_iter=50, scoring='accuracy', cv=5, random_state=0)\n",
    "    optimizer.fit(features, labels)\n",
    "    best_params = optimizer.best_params_\n",
    "    best_score = optimizer.best_score_\n",
    "    \n",
    "    # Get scores for all metrics using cross_val_score\n",
    "    mean_acc = cross_val_score(optimizer.best_estimator_, features, labels, scoring='accuracy', cv=5).mean()\n",
    "    std_acc = cross_val_score(optimizer.best_estimator_, features, labels, scoring='accuracy', cv=5).std()\n",
    "    mean_precision = cross_val_score(optimizer.best_estimator_, features, labels, scoring='precision_micro', cv=5).mean()\n",
    "    std_precision = cross_val_score(optimizer.best_estimator_, features, labels, scoring='precision_micro', cv=5).std()\n",
    "    mean_recall = cross_val_score(optimizer.best_estimator_, features, labels, scoring='recall_micro', cv=5).mean()\n",
    "    std_recall = cross_val_score(optimizer.best_estimator_, features, labels, scoring='recall_micro', cv=5).std()\n",
    "    mean_f1 = cross_val_score(optimizer.best_estimator_, features, labels, scoring='f1_micro', cv=5).mean()\n",
    "    std_f1 = cross_val_score(optimizer.best_estimator_, features, labels, scoring='f1_micro', cv=5).std()\n",
    "\n",
    "    print(f\"Best parameters for {name}: {best_params}\")\n",
    "    print(f\"Best CV score for {name} (Accuracy): {best_score:.4f}\")\n",
    "    return (mean_acc, std_acc, mean_precision, std_precision, mean_recall, std_recall, mean_f1, std_f1)\n",
    "\n",
    "# Prepare a list to collect results\n",
    "results_list = []\n",
    "\n",
    "# Evaluate each model with and without PCA\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name} without PCA...\")\n",
    "    results_without_pca = evaluate_model(name, model, features_scaled, encoded_labels)\n",
    "    results_list.append({'Model': name, 'Feature Set': 'Without PCA', \n",
    "                         'Accuracy': results_without_pca[0], 'Acc Std': results_without_pca[1],\n",
    "                         'Precision': results_without_pca[2], 'Prec Std': results_without_pca[3],\n",
    "                         'Recall': results_without_pca[4], 'Recall Std': results_without_pca[5],\n",
    "                         'F1 Score': results_without_pca[6], 'F1 Std': results_without_pca[7]})\n",
    "    \n",
    "    print(f\"Evaluating {name} with PCA...\")\n",
    "    results_with_pca = evaluate_model(name, model, features_pca, encoded_labels)\n",
    "    results_list.append({'Model': name, 'Feature Set': 'With PCA', \n",
    "                         'Accuracy': results_with_pca[0], 'Acc Std': results_with_pca[1],\n",
    "                         'Precision': results_with_pca[2], 'Prec Std': results_with_pca[3],\n",
    "                         'Recall': results_with_pca[4], 'Recall Std': results_with_pca[5],\n",
    "                         'F1 Score': results_with_pca[6], 'F1 Std': results_with_pca[7]})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "results_table = pd.DataFrame(results_list)\n",
    "\n",
    "# Save the results table to a CSV file\n",
    "results_csv_path = '../data/results_table.csv'\n",
    "results_table.to_csv(results_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Model Performance\n",
    "\n",
    "In this section, we visualize the performance of different machine learning models evaluated with and without Principal Component Analysis (PCA). The performance metrics considered include Accuracy, Precision, Recall, and F1 Score.\n",
    "\n",
    "#### Plot Configuration\n",
    "\n",
    "- **Setup**: We use the `seaborn` and `matplotlib` libraries to create clear and informative bar plots, depicting each model's performance across the four metrics.\n",
    "- **Data Source**: The results are read from the previously saved CSV file, ensuring consistency between the reported results and the visualization.\n",
    "\n",
    "#### Metrics Plotted\n",
    "\n",
    "- **Accuracy**: Reflects the overall correctness of the model's predictions.\n",
    "- **Precision (Micro-averaged)**: Indicates the proportion of correctly predicted positive observations.\n",
    "- **Recall (Micro-averaged)**: Measures the model's ability to identify all relevant instances.\n",
    "- **F1 Score (Micro-averaged)**: Combines precision and recall into a single metric, emphasizing balance.\n",
    "\n",
    "#### Plot Details\n",
    "\n",
    "- **Subplots**: We generate a 2x2 grid of subplots, each displaying one of the metrics for all models and feature sets.\n",
    "- **Error Bars**: Standard deviation values are used to depict confidence intervals for each metric, providing insight into the variability of model performance across cross-validation folds.\n",
    "- **Legend**: A consolidated legend outside the plot area clarifies which feature set (with or without PCA) corresponds to each bar, enhancing the plot's readability.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "This visualization allows for a straightforward comparison of the models, highlighting the impact of PCA on their performance across various metrics. It serves as a valuable tool for identifying which model and feature set combination yields the best balance between accuracy, precision, recall, and F1 score.\n",
    "\n",
    "By visualizing these metrics, we can quickly discern patterns and differences in performance, enabling informed decisions about model selection and feature engineering in the context of bearing fault classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "results_csv_path = '../data/results_table.csv'\n",
    "results_table = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Plot configuration\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Define the metrics to plot\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "std_metrics = ['Acc Std', 'Prec Std', 'Recall Std', 'F1 Std']\n",
    "\n",
    "# Iterate over the metrics to create subplots\n",
    "for i, (metric, std_metric) in enumerate(zip(metrics, std_metrics)):\n",
    "    plt.subplot(2, 2, i + 1)  # Create a subplot for each metric\n",
    "    ax = sns.barplot(\n",
    "        x='Model',\n",
    "        y=metric,\n",
    "        hue='Feature Set',\n",
    "        data=results_table,\n",
    "        palette='muted',\n",
    "        errorbar=None  # we will add it manually\n",
    "    )\n",
    "\n",
    "    # Add error bars manually\n",
    "    for container in ax.containers:\n",
    "        if isinstance(container, plt.matplotlib.container.BarContainer):\n",
    "            bar_centers = [bar.get_x() + bar.get_width() / 2 for bar in container]\n",
    "            ax.errorbar(bar_centers, container.datavalues, yerr=results_table[std_metric].values[:len(container)],\n",
    "                        fmt='none', c='black', capsize=5, linewidth=1.5)\n",
    "\n",
    "    plt.title(f'{metric}', fontsize=16)\n",
    "    plt.xlabel('Model', fontsize=14)\n",
    "    plt.ylabel(metric, fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "# Place a single legend outside the plot\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.figlegend(handles, labels, loc='upper center', ncol=2, title='Feature Set', bbox_to_anchor=(0.5, 0.01), fontsize=12, title_fontsize=14)\n",
    "\n",
    "# Adjust the layout to make room for the legend\n",
    "plt.tight_layout(rect=[0, 0.04, 1, 0.95])\n",
    "\n",
    "# Add a main title\n",
    "plt.suptitle('Model Performance Comparison with and without PCA', fontsize=18)\n",
    "\n",
    "# Define the path for the results directory\n",
    "results_dir = '../Results/ML_Performances'\n",
    "\n",
    "# Create the Results directory if it doesn't exist\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, 'ML_Performances.png'))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
